#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Mar  3 18:35:52 2019

@author: Philip Prohaska

Title: Linear Modeling Case Study

Purpose: Data analysis and analysis code

"""
#Working Directory:


# Loading Libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

file = 'birthweight_feature_set.xlsx'

birthweight = pd.read_excel(file)

########################
# Fundamental Dataset Exploration
########################

# Column names
birthweight.columns

# Displaying the first rows of the DataFrame
print(birthweight.head())

# Dimensions of the DataFrame
birthweight.shape

# Information about each variable
birthweight.info()

# Descriptive statistics
birthweight.describe().round(2)

birthweight.sort_values('bwght', ascending = False)

###############################################################################
# Imputing Missing Values
###############################################################################

print(
      birthweight
      .isnull()
      .sum()
      )

for col in birthweight:

    """ Create columns that are 0s if a value was not missing and 1 if
    a value is missing. """
    
    if birthweight[col].isnull().any():
        birthweight['m_'+col] = birthweight[col].isnull().astype(int)


# Biggest number of missing values is in meduc, npvis and feduc

# Creating a dropped dataset to graph for all columns with missing values
df_dropped = birthweight.dropna()

#Mother educ age not normally distributed -> median
sns.distplot(df_dropped['meduc'])

# Total num of prental visits normally distrib -> mean
sns.distplot(df_dropped['npvis'])

# Father educ not normally distrib -> median 
sns.distplot(df_dropped['feduc'])

# Filled with the median
fill_meduc = birthweight['meduc'].median()
birthweight['meduc'] = birthweight['meduc'].fillna(fill_meduc)

fill_feduc = birthweight['feduc'].median()
birthweight['feduc'] = birthweight['feduc'].fillna(fill_feduc)

# Filled with the mean
fill_npvis = birthweight['npvis'].mean()
birthweight['npvis'] = birthweight['npvis'].fillna(fill_npvis)

# Checking the overall dataset to see if there are any remaining
# missing values
print(
      birthweight
      .isnull()
      .any()
      .any()
      )

###############################################################################
# Outlier Analysis
###############################################################################

birthweight_quantiles = birthweight.loc[:, :].quantile([0.20,
                                                0.40,
                                                0.60,
                                                0.80,
                                                1.00])
    
print(birthweight_quantiles)

for col in birthweight:
    print(col)

"""
Assumed Continuous/Interval Variables - 

mage
meduc
monpre
npvis
fage
feduc
omaps
fmaps
cigs
drink
bwght

Binary Classifiers -

male
mwhte
mblck
moth
fwhte
fblck
foth
m_meduc
m_npvis
m_feduc
"""

#### After research we decided to delete post- pregnancy variables  (omaps & fmaps)
########################
# Visual EDA (Histograms)
########################
plt.subplot(4, 2, 1)
sns.distplot(birthweight['meduc'],
             bins = 'fd',
             color = 'g')

plt.xlabel('meduc')
########################
plt.subplot(4, 2, 2)
sns.distplot(birthweight['npvis'],
             bins = 'fd',
             color = 'g')

plt.xlabel('npvis')
########################
plt.subplot(4, 2, 3)
sns.distplot(birthweight['feduc'],
             bins = 'fd',
             color = 'g')

plt.xlabel('feduc')
########################
plt.subplot(4, 2, 4)
sns.distplot(birthweight['monpre'],
             bins = 'fd',
             color = 'g')

plt.xlabel('monpre')
########################
plt.subplot(4, 2, 5)
sns.distplot(birthweight['mage'],
             bins = 'fd',
             color = 'g')

plt.xlabel('mage')
########################
plt.subplot(4, 2, 6)
sns.distplot(birthweight['cigs'],
             bins = 'fd',
             color = 'g')

plt.xlabel('cigs')
########################
plt.subplot(4, 2, 7)
sns.distplot(birthweight['drink'],
             bins = 'fd',
             color = 'g')

plt.xlabel('drink')
########################
plt.subplot(4, 2, 8)
sns.distplot(birthweight['fage'],
             bins = 'fd',
             color = 'g')

plt.xlabel('fage')
########################
# Tuning and Flagging Outliers
########################

birthweight_quantiles = birthweight.loc[:, :].quantile([0.05,
                                                0.40,
                                                0.60,
                                                0.80,
                                                0.95])

# Outlier flags

overall_npvis_hi    = 20

feduc_lo    = 7


########################
# Creating Outlier Flags
########################

# Building loops for outlier imputation
########################
# Npvis
     
birthweight['out_npvis'] = 0

        
for val in enumerate(birthweight.loc[ : , 'npvis']):
    
    if val[1] >= overall_npvis_hi:
        birthweight.loc[val[0], 'out_npvis'] = 1

# Feduc

birthweight['out_feduc'] = 0


for val in enumerate(birthweight.loc[ : , 'feduc']):
    
    if val[1] <= feduc_lo:
        birthweight.loc[val[0], 'out_feduc'] = -1

###############################################################################
# Correlation Analysis
###############################################################################

birthweight.head()


df_corr = birthweight.corr().round(2)


print(df_corr)


df_corr.loc['bwght'].sort_values(ascending = False)

########################
# Correlation Heatmap
########################
# Using palplot to view a color scheme
sns.palplot(sns.color_palette('coolwarm', 12))

fig, ax = plt.subplots(figsize=(15,15))

df_corr2 = df_corr.iloc[1:18, 1:18]

sns.heatmap(df_corr2,
            cmap = 'coolwarm',
            square = True,
            annot = True,
            linecolor = 'black',
            linewidths = 0.5)


plt.savefig('Birthweight Heatmap.png')
plt.show()

birthweight.to_excel('Birth_explored.xlsx')

# The higest correlation between our DV is with cigarettes and drinks 
# negativer correlation - the more cigs/drinks consumed the lower the birthweight

########################
# OLS Regression
########################

# Loading Libraries
import statsmodels.formula.api as smf

###############################################################################
# Univariate Regression Analysis
###############################################################################

# Building a Regression Base
lm_bwght_drink = smf.ols(formula = """bwght ~ birthweight['drink']""",
                         data = birthweight)

# Fitting Results
results = lm_bwght_drink.fit()

# Printing Summary Statistics
print(results.summary())

print(f"""
Parameters:
{results.params.round(2)}

Summary Statistics:
R-Squared:          {results.rsquared.round(3)}
Adjusted R-Squared: {results.rsquared_adj.round(3)}
""")

########################
# Full Model
########################
    
 #Selecting varaibles which are the mostly correlated

lm_full = smf.ols(formula = """bwght ~ birthweight['mage'] +
                                           birthweight['meduc'] +
                                           birthweight['monpre'] +
                                           birthweight['npvis'] +
                                           birthweight['fage'] +
                                           birthweight['feduc'] +
                                           birthweight['cigs'] +
                                           birthweight['drink'] +
                                           birthweight['male'] +
                                           birthweight['mwhte'] +
                                           birthweight['mblck'] +
                                           birthweight['moth'] +
                                           birthweight['fwhte'] +
                                           birthweight['fblck'] +
                                           birthweight['out_npvis'] +
                                           birthweight['out_feduc'] +
                                           birthweight['foth'] -1
                                           """,
                         data = birthweight)

# Fitting Results
results = lm_full.fit()

# Printing Summary Statistics
print(results.summary())

print(f"""
Summary Statistics:
R-Squared:          {results.rsquared.round(3)}
Adjusted R-Squared: {results.rsquared_adj.round(3)}
""")
     
# We are now feature engeneering the racial dependencies
# Grouping both parent by race - Black
import numpy as np

birthweight['parent_black'] = 0

birthweight['parent_black'] = np.logical_and(birthweight['fblck'] == 1,
           birthweight['mblck'] == 1)

# Translating data to Interger 
birthweight['parent_black_int'] = 0

for val in enumerate(birthweight.loc[ : , 'parent_black']):
    
    if val[1] == True:
        birthweight.loc[val[0], 'parent_black_int'] = 1


# Grouping both parent by race - White
birthweight['parent_white'] = 0

birthweight['parent_white'] = np.logical_and(birthweight['fwhte'] == 1,
           birthweight['mwhte'] == 1)

# Translating data to Interger 
birthweight['parent_white_int'] = 0

for val in enumerate(birthweight.loc[ : , 'parent_white']):
    
    if val[1] == True:
        birthweight.loc[val[0], 'parent_white_int'] = 1

# Grouping both parent by race - Other
birthweight['parent_other'] = 0

birthweight['parent_other'] = np.logical_and(birthweight['foth'] == 1,
           birthweight['moth'] == 1)

# Translating data to Interger 
birthweight['parent_other_int'] = 0

for val in enumerate(birthweight.loc[ : , 'parent_other']):
    
    if val[1] == True:
        birthweight.loc[val[0], 'parent_other_int'] = 1
    
    
## Exploring model with race added groupped variables

race_full = smf.ols(formula = """bwght ~ birthweight['mage'] +
                                           birthweight['meduc'] +
                                           birthweight['monpre'] +
                                           birthweight['npvis'] +
                                           birthweight['fage'] +
                                           birthweight['feduc'] +
                                           birthweight['cigs'] +
                                           birthweight['drink'] +
                                           birthweight['male'] +
                                           birthweight['mwhte'] +
                                           birthweight['mblck'] +
                                           birthweight['moth'] +
                                           birthweight['fwhte'] +
                                           birthweight['fblck'] +
                                           birthweight['out_npvis'] +
                                           birthweight['out_feduc'] +
                                           birthweight['foth'] +
                                           birthweight['parent_white_int'] +
                                           birthweight['parent_black_int'] +
                                           birthweight['parent_other_int'] -1
                                                  """,
                                                  data = birthweight)
# Fitting Results
results_race = race_full.fit()

# Printing Summary Statistics
print(results_race.summary())  
    
######### Grouping mage ############

birthweight['mage_group'] = 0

bins = [20, 26, 31, 36, 41, 46, 51, 55, 100]
labels = ['M20-25', 'M26-30', 'M31-35', 'M36-40','M41-45','M46-50','M51-55', 'M56+']
birthweight['mage_group'] = pd.cut(birthweight.mage, bins, labels = labels,include_lowest = True)

######### Grouping fage ###########

birthweight['fage_group'] = 0

bins = [20, 26, 31, 36, 41, 46, 51, 100]
labels = ['F20-25', 'F26-30', 'F31-35', 'F36-40','F41-45','F46-50', 'F51+']
birthweight['fage_group'] = pd.cut(birthweight.fage, bins, labels = labels,include_lowest = True)
 
######### Grouping feduc ###########

birthweight['feduc_group'] = 0

bins = [0, 13, 17]
labels = ['F1', 'F2']
birthweight['feduc_group'] = pd.cut(birthweight.feduc, bins, labels = labels,include_lowest = True)
 
######### Grouping meduc ###########

birthweight['meduc_group'] = 0

bins = [1, 13, 17]
labels = ['M1', 'M2']
birthweight['meduc_group'] = pd.cut(birthweight.meduc, bins, labels = labels,include_lowest = True)  
    
########################
# Working with Categorical Variables of age and education 
########################

# One-Hot Encoding Qualitative Variables
mage_group_dummies = pd.get_dummies(list(birthweight['mage_group']), drop_first = True)
fage_group_dummies = pd.get_dummies(list(birthweight['fage_group']), drop_first = True)
meduc_group_dummies = pd.get_dummies(list(birthweight['meduc_group']), drop_first = True)
feduc_group_dummies = pd.get_dummies(list(birthweight['feduc_group']), drop_first = True)

# Concatenating One-Hot Encoded Values with the Larger DataFrame
birthweight_2 = pd.concat(
        [birthweight.loc[:,:],
         mage_group_dummies, fage_group_dummies, meduc_group_dummies, feduc_group_dummies],
         axis = 1)

birthweight_2.columns
 ## Exploring model with dummies
   
lm2_full = smf.ols(formula = """bwght ~ birthweight_2['mage'] +            
                                           birthweight_2['monpre'] +
                                           birthweight_2['npvis'] +
                                           birthweight_2['fage'] +
                                           birthweight_2['feduc'] +
                                           birthweight_2['cigs'] +
                                           birthweight_2['drink'] +
                                           birthweight_2['male'] +
                                           birthweight_2['mwhte'] +
                                           birthweight_2['mblck'] +
                                           birthweight_2['moth'] +
                                           birthweight_2['fwhte'] +
                                           birthweight_2['fblck'] + 
                                           birthweight_2['foth']+
                                           birthweight_2['out_npvis']+
                                           birthweight_2['out_feduc']+
                                           birthweight_2['parent_white_int'] +
                                           birthweight_2['parent_black_int'] +
                                           birthweight_2['parent_other_int'] +
                                           birthweight_2['fage_group'] +
                                           birthweight_2['feduc_group'] +
                                           birthweight_2['meduc_group'] +
                                           birthweight_2['M26-30'] +
                                           birthweight_2['M31-35'] +
                                           birthweight_2['M36-40'] +
                                           birthweight_2['M41-45'] +
                                           birthweight_2['M46-50'] +
                                           birthweight_2['M51-55'] +
                                           birthweight_2['M56+'] +
                                           birthweight_2['F26-30'] +
                                           birthweight_2['F31-35'] +
                                           birthweight_2['F36-40'] +
                                           birthweight_2['F41-45'] +
                                           birthweight_2['F46-50'] +
                                           birthweight_2['F51+'] +
                                           birthweight_2['M2'] +
                                           birthweight_2['F2'] +
                                           birthweight_2['mage_group']-1
                                                  """,
                                                  data = birthweight_2)
# Fitting Results
results_lm2 = lm2_full.fit()

# Printing Summary Statistics
print(results_lm2.summary())

### Removing insignificant values 

lm3_full = smf.ols(formula = """bwght ~            
                                           birthweight_2['cigs'] +
                                           birthweight_2['drink'] +
                                           birthweight_2['parent_black_int'] +
                                           birthweight_2['parent_white_int'] +
                                           birthweight_2['parent_other_int'] +
                                           birthweight_2['M26-30'] +
                                           birthweight_2['M31-35'] +
                                           birthweight_2['M36-40'] +
                                           birthweight_2['M41-45'] +
                                           birthweight_2['M46-50'] +
                                           birthweight_2['M51-55'] +
                                           birthweight_2['M56+'] +
                                           birthweight_2['F26-30'] +
                                           birthweight_2['F31-35'] +
                                           birthweight_2['F36-40'] +
                                           birthweight_2['F41-45'] +
                                           birthweight_2['F46-50'] +
                                           birthweight_2['M2'] -1
                                                  """,
                                                  data = birthweight_2)
# Fitting Results
results_lm3 = lm3_full.fit()

# Printing Summary Statistics
print(results_lm3.summary())

#we see P value incerease as the mothers age increases

print(f"""
Summary Statistics:
R-Squared:          {results_lm3.rsquared.round(3)}
Adjusted R-Squared: {results_lm3.rsquared_adj.round(3)}
""")
    
# Checking predicted birthweight vs. actual birth weight
predict = results_lm3.predict()
y_hat   = pd.DataFrame(predict).round(2)
resids  = results.resid.round(2)

# Plotting residuals
residual_analysis = pd.concat(
        [birthweight_2.loc[:,'bwght'],
         y_hat,
         results.resid.round(2)],
         axis = 1)

residual_analysis.to_excel('SomethingNew.xlsx')

sns.residplot(x = predict,
              y = birthweight_2.loc[:,'bwght'])

plt.show()

# Saving as a new dataset for future use.
birthweight_2.to_excel('Birthweight_Dummies.xlsx')

###########################
#Prediction Models
###########################

# Importing new libraries
from sklearn.model_selection import train_test_split # train/test split
from sklearn.neighbors import KNeighborsRegressor # KNN for Regression
import statsmodels.formula.api as smf # regression modeling
import pandas as pd
import matplotlib.pyplot as plt

file_dum = 'Birthweight_Dummies.xlsx'

birthweight_dum = pd.read_excel(file_dum)

###############################################################################
# Generalization using Train/Test Split
###############################################################################

#Setting up our dataset a little differently by separating the features
# from the independent variable.

# Also, keeping in mind that many machine learning models do not handle categorical
# data very well. We can use df.drop() to eliminate such features.

birthweight_data = birthweight_dum.drop(['bwght',
                                         'parent_black',
                                         'parent_white',
                                         'parent_other',
                                         'mage_group',
                                         'fage_group',
                                         'feduc_group',
                                         'meduc_group'],
                                          axis = 1)

birthweight_target = birthweight_dum.loc[:, 'bwght']

X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target)

# Let's check to make sure our shapes line up.

# Training set 
print(X_train.shape)
print(y_train.shape)

# Testing set
print(X_test.shape)
print(y_test.shape)


"""
    By default, 75% of our data went to the training dataset, while 25% went to
    the testing dataset, we can change this using the argument test_size. We are
    keeping it a default level.
"""

X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target,
            test_size = 0.25,
            random_state = 508)

# Checking shapes again.

# Training set 
print(X_train.shape)
print(y_train.shape)

# Testing set
print(X_test.shape)
print(y_test.shape)

###############################################################################
# Forming a Base for Machine Learning with KNN
###############################################################################
# Step 1: Create a model object

# Creating a regressor object
knn_reg = KNeighborsRegressor(algorithm = 'auto',
                              n_neighbors = 25)

# Checking the type of this new object
type(knn_reg)

# Teaching (fitting) the algorithm based on the training data
knn_reg.fit(X_train, y_train)

# Predicting on the X_data that the model has never seen before
y_pred = knn_reg.predict(X_test)

# Printing out prediction values for each test observation
print(f"""
Test set predictions:
{y_pred}
""")

# Calling the score method, which compares the predicted values to the actual
# values
y_score = knn_reg.score(X_test, y_test)

# The score is directly comparable to R-Square
print(y_score)

###############################################################################
# How Many Neighbors? Selecting best KNN value
###############################################################################

# This is the exact code we were using before
X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target,
            test_size = 0.10,
            random_state = 508)

# Creating two lists, one for training set accuracy and the other for test
# set accuracy
training_accuracy = []
test_accuracy = []

# Building a visualization to check to see  1 to 50
neighbors_settings = range(1, 10)


for n_neighbors in neighbors_settings:
    # Building the model
    clf = KNeighborsRegressor(n_neighbors = n_neighbors)
    clf.fit(X_train, y_train)
    
    # Recording the training set accuracy
    training_accuracy.append(clf.score(X_train, y_train))
    
    # Recording the generalization accuracy
    test_accuracy.append(clf.score(X_test, y_test))


# Plotting the visualization
fig, ax = plt.subplots(figsize=(12,9))
plt.plot(neighbors_settings, training_accuracy, label = "training accuracy")
plt.plot(neighbors_settings, test_accuracy, label = "test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()
plt.show()

print(max(test_accuracy))

# The best results occur when k = 4
# Building a model with k = 4
knn_reg = KNeighborsRegressor(algorithm = 'auto',
                              n_neighbors = 4)

# Fitting the model based on the training data
knn_reg.fit(X_train, y_train)

# Scoring the model
y_score = knn_reg.score(X_test, y_test)

# The score is directly comparable to R-Square
print(y_score)

print(f"""
Our base to compare other models is {y_score.round(3)}.
    
This base helps us evaluate more complicated models and lets us consider
tradeoffs between accuracy and interpretability.
""")

###############################################################################
# OLS Regression Analysis in statsmodels
###############################################################################
"""
    Now that we have a learning base, we can develop a base model using OLS
    linear regression. Our goal in this step is to discover which parameters
    have the strongest effects on our response variable.
"""
# This is the exact code we were using before
X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target,
            test_size = 0.10,
            random_state = 508)

# We need to merge our X_train and y_train sets so that they can be
# used in statsmodels
birthweight_train = pd.concat([X_train, y_train], axis = 1)

# Review of statsmodels.ols

# Step 1: Build the model
lm_bwght_qual = smf.ols(formula = """bwght ~
                                     birthweight_train['drink']""",
                         data = birthweight_train)

# Step 2: Fit the model based on the data
results = lm_bwght_qual.fit()

# Step 3: Analyze the summary output
print(results.summary())

# Let's pull in the optimal model from before, only this time on the training
# set. In OLS original model we gropped parents by race to increase the value 
# of R-square by descreasing variance. But for the following model, it was not
# enough data to train the algorythm so we decided to use the original binomial
# race variables. We did not do it for age or education as race was enough and 
# once we tried this, the predictability score decreased.
lm_significant = smf.ols(formula = """bwght ~            
                                           birthweight_train['cigs'] +
                                           birthweight_train['drink'] +
                                           birthweight_train['mwhte'] +
                                           birthweight_train['mblck'] +
                                           birthweight_train['moth']+
                                           birthweight_train['fwhte']+
                                           birthweight_train['fblck']+
                                           birthweight_train['foth']+
                                           birthweight_train['M26-30'] +
                                           birthweight_train['M31-35'] +
                                           birthweight_train['M36-40'] +
                                           birthweight_train['M41-45'] +
                                           birthweight_train['M46-50'] +
                                           birthweight_train['M51-55'] +
                                           birthweight_train['M56+'] +
                                           birthweight_train['F26-30'] +
                                           birthweight_train['F31-35'] +
                                           birthweight_train['F36-40'] +
                                           birthweight_train['F41-45'] +
                                           birthweight_train['F46-50'] -1
                                                  """,
                                                  data = birthweight_train)


# Fitting Results
results = lm_significant.fit()

# Printing Summary Statistics
print(results.summary())

# Now that we have selected our variables, our next step is to prepare them
# in scikit-learn so that we can see how they predict on new data.

###############################################################################
# Applying the Optimal Model in scikit-learn
###############################################################################

# Preparing a DataFrame based the the analysis above
birthweight_data = birthweight_dum.loc[:,['cigs',
                                          'drink',
                                          'mwhte',
                                          'mblck',
                                          'moth',
                                          'fwhte',
                                          'fblck',
                                          'M26-30',
                                          'M31-35',
                                          'M36-40',
                                          'M41-45',
                                          'M46-50',
                                          'M56+',
                                          'F26-30',
                                          'F31-35',
                                          'F36-40',
                                          'F41-45',
                                          'F46-50']]

# Preparing the target variable
birtweight_target = birthweight_dum.loc[:, 'bwght']

X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target,
            test_size = 0.10,
            random_state = 508)

########################
# Using KNN  on the optimal model (same code as before)
########################

# Exact loop as before
training_accuracy = []
test_accuracy = []

neighbors_settings = range(1, 50)

for n_neighbors in neighbors_settings:
    # build the model
    clf = KNeighborsRegressor(n_neighbors = n_neighbors)
    clf.fit(X_train, y_train)
    
    # record training set accuracy
    training_accuracy.append(clf.score(X_train, y_train))
    
    # record generalization accuracy
    test_accuracy.append(clf.score(X_test, y_test))

plt.plot(neighbors_settings, training_accuracy, label = "training accuracy")
plt.plot(neighbors_settings, test_accuracy, label = "test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()

print(max(test_accuracy))
test_accuracy.index(0.5993223255193767)

########################
# The best results occur when k = 11.
########################

# Building a model with k = 11
knn_reg = KNeighborsRegressor(algorithm = 'auto',
                              n_neighbors = 11)

# Fitting the model based on the training data
knn_reg_fit = knn_reg.fit(X_train, y_train)

# Scoring the model
y_score_knn_optimal = knn_reg.score(X_test, y_test)

# The score is directly comparable to R-Square
print(y_score_knn_optimal)

# Generating Predictions based on the optimal KNN model
knn_reg_optimal_pred = knn_reg_fit.predict(X_test)

########################
## Checking if OLS predicts better than KNN
########################

from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = train_test_split(
            birthweight_data,
            birthweight_target,
            test_size = 0.10,
            random_state = 508)


# Prepping the Model
lr = LinearRegression(fit_intercept = False)

# Fitting the model
lr_fit = lr.fit(X_train, y_train)

# Predictions
lr_pred = lr_fit.predict(X_test)

print(f"""
Test set predictions:
{y_pred.round(2)}
""")

# Scoring the model
y_score_ols_optimal = lr_fit.score(X_test, y_test)

# The score is directly comparable to R-Square
print(y_score_ols_optimal)

# Let's compare the testing score to the training score.

print('Training Score', lr.score(X_train, y_train).round(4))
print('Testing Score:', lr.score(X_test, y_test).round(4))

# Printing model results
print(f"""
Full model KNN score:    {y_score.round(3)}
Optimal model KNN score: {y_score_knn_optimal.round(3)}
Optimal model OLS score: {y_score_ols_optimal.round(3)}
""")

# Both the KNN and the OLS models have similar predicatibility scores.
# OLS is an example of a parametric approach as it assumes linear relationship
# whereas KNN in a non-parametric method and provides a more flexible approach.
# If there is a smaller number of observations per predictor KNN
# does not perform as well as OLS, and it may be because OLS is a bit better
# in predicting birthweight in our limited data set 
#(train = approx. 10 observations per paramiter)
